# ML Model Service optimized for OpenStack deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-model-openstack
  namespace: sentiment-analyzer
  labels:
    app: ml-model
    version: v1
    platform: openstack
spec:
  replicas: 2  # Start with 2 replicas on dedicated ML worker
  selector:
    matchLabels:
      app: ml-model
      version: v1
      platform: openstack
  template:
    metadata:
      labels:
        app: ml-model
        version: v1
        platform: openstack
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8001"
        prometheus.io/path: "/metrics"
    spec:
      # Schedule on ML-optimized worker node
      nodeSelector:
        kubernetes.io/hostname: k8s-worker-2
      # Node affinity for high-memory workloads
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: node-type
                operator: In
                values:
                - ml-worker
      containers:
      - name: ml-model
        image: sentiment-analyzer/ml-model:openstack
        ports:
        - containerPort: 8001
          name: http
        env:
        - name: PYTHONPATH
          value: "/app"
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: sentiment-analyzer-config
              key: LOG_LEVEL
        - name: TRANSFORMERS_CACHE
          value: "/app/model_cache"
        - name: HF_HOME
          value: "/app/model_cache"
        # OpenStack VM resource allocation
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        # Health checks with longer timeouts for ML model loading
        livenessProbe:
          httpGet:
            path: /health
            port: 8001
          initialDelaySeconds: 120  # ML model needs time to load
          periodSeconds: 30
          timeoutSeconds: 15
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8001
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 10
          failureThreshold: 3
        volumeMounts:
        - name: model-cache
          mountPath: /app/model_cache
        - name: tmp-storage
          mountPath: /tmp
      # Pre-download model in init container
      initContainers:
      - name: model-downloader
        image: sentiment-analyzer/ml-model:openstack
        command: 
        - python
        - -c
        - |
          import os
          os.environ['TRANSFORMERS_CACHE'] = '/app/model_cache'
          os.environ['HF_HOME'] = '/app/model_cache'
          from transformers import pipeline
          print("Downloading DistilBERT model...")
          pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")
          print("Model download complete")
        volumeMounts:
        - name: model-cache
          mountPath: /app/model_cache
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
      volumes:
      - name: model-cache
        emptyDir:
          sizeLimit: 2Gi
      - name: tmp-storage
        emptyDir:
          sizeLimit: 1Gi
      # Topology spread for better distribution
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: ml-model
---
apiVersion: v1
kind: Service
metadata:
  name: ml-model-service-openstack
  namespace: sentiment-analyzer
  labels:
    app: ml-model
    platform: openstack
spec:
  selector:
    app: ml-model
    platform: openstack
  ports:
  - port: 8001
    targetPort: 8001
    protocol: TCP
    name: http
  type: ClusterIP
  # Session affinity for better performance
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 300
---
# Horizontal Pod Autoscaler optimized for OpenStack
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-model-hpa-openstack
  namespace: sentiment-analyzer
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-model-openstack
  minReplicas: 2
  maxReplicas: 6  # Limited by single ML worker node capacity
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 20
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
---
# Pod Disruption Budget for high availability
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ml-model-pdb
  namespace: sentiment-analyzer
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: ml-model
      platform: openstack